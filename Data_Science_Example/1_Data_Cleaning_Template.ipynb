{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18c7e3a0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Notebook Author:<br>Felix Gonzalez, P.E. <br> Adjunct Instructor, <br> Division of Professional Studies <br> Computer Science and Electrical Engineering <br> University of Maryland Baltimore County <br> fgonzale@umbc.edu\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f867845e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Acknowledgements:<br>\n",
    "This dataset was generated from The Movie Database API (https://www.kaggle.com/datasets/tmdb/themoviedb.org). This product uses the TMDb API but is not endorsed or certified by TMDb. Their API also provides access to data on many additional movies, actors and actresses, crew members, and TV shows. You can try it for yourself here: https://www.themoviedb.org/documentation/api.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2ac0d8",
   "metadata": {},
   "source": [
    "# Data Cleaning Template\n",
    "This notebook can be used as a template for performing data cleaning of any data. Once the data is loaded as a dataframe (DF), most functions can be used as is with minimal modifications. Only functions that call a feature or column name would need to be modified as appropriate. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6fca13",
   "metadata": {},
   "source": [
    "# Analysis Goal\n",
    "This dataset was selected as an example because it included a various data types including various forms of nominal, qualitative, quantitative and continous data types in the features. For example, the dataset includes examples of structured data such as currency related features (e.g., budget, revenue), categorical features (e.g., genre, original language), numerical features (e.g., runtime), dates (e.g., release dates), entity/organization features (e.g., production company, production country), as well as unstructured data (e.g., title, overview). Other example data types that are NOT in this dataset include geographical information (e.g., coordinates such as latitude and longitude), audio, speech, video, and pictures.\n",
    "\n",
    "All of these can be used to perform various analytical tasks from calculating new derived features, simple visualizations to ML classification. For example:\n",
    "- Could use budget and revenue to calculate a revenue/budget ratio. If the revenue = box_office_earning-budget we can calculate the box_office_earnings = revenue+budget. Other derived features that may be of interest and could be calculated wiht the data could be Return of Investment. These would be derived (i.e., calculated) features. A subject matter expert (SME) familiar with the data may be able to provide further advice on which metrics would be useful. \n",
    "- Could use the release dates and revenue (or budget) to trend the revenue per year. Note that the date does not seem to provide information if the currency data is adjusted for inflation. Adjusting for inflation may be addressed by obtaining historical inflation and adjusting the dataset.\n",
    "- Could use the unstructured data to create a classification system and classify the genre of a new movie (i.e., by reading its overview). This would be similar to a new email being classified as SPAM or HAM.\n",
    "- The unstructued data could be used to perform topic analysis including but not limited to wordclouds, text clustering, topic modeling (e.g., LDA)\n",
    "\n",
    "The example in this Jupyter Notebook take into consideration all of the potential applications above when performing the data cleaning and addressing common or typical data issues. Common typical data issues include but is not limited to:\n",
    "- Duplicate values\n",
    "- Null values\n",
    "- Data entry errors\n",
    "    - Missclassifications\n",
    "    - Wrong values\n",
    "    - Negative values where only positive values should exist\n",
    "    - String values where numerical values should exist (and viceversa)\n",
    "    - Errors were data may be orders of magnitude above/below expected\n",
    "\n",
    "Examples of these errors and issues are further explored in this notebook. \n",
    "\n",
    "Some data entry errors can be minimized during the data entry stage and system user interface. For example, in a data entry system, if we have a field on 'person age', we can design the system to only accept values between 0 and 150. This range should  cover all known age ranges, minimize errors in that feature and increase the quality of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c15915",
   "metadata": {},
   "source": [
    "# Source Data Description\n",
    "The original dataset and full detail of the description can be found at https://www.kaggle.com/datasets/tmdb/tmdb-movie-metadata (Last accessed 10/5/2022). \n",
    "\n",
    "Kaggle The Movie Database (TMDb) uploaded in accordance with their terms of use. TMDb movie dataset notes:\n",
    "- The original dataset is from The Movie Database (TMDb). The terms of use can be found at https://www.themoviedb.org/documentation/api/terms-of-use. Their application programming interface (API) can be found at https://www.themoviedb.org/documentation/api/.\n",
    "- Actor and actresses seem to be listed in the order they appear in the credits.\n",
    "- There seems to be a few movies that did not port correctly.\n",
    "- Several columns contain json. \n",
    "- Due to the different amount of movie versions (e.g., theater release, directors cut, etc.) there may be differences in runtime between this movie dataset and others.\n",
    "- All fields are filled out by users and there is the potential for disagreement or missclassification in keywords, genres, ratings, or the like.\n",
    "\n",
    "There are a few open questions in the dataset that include:\n",
    "- Are the budgets and revenues all in US dollars? Do they consistently show the global revenues?\n",
    "- This dataset hasn't yet gone through a data quality analysis. For example, in the previous version of the movie dataset (i.e., from IMDb), it was necessary to treat values of zero in the budget field as missing. It's probably a good idea to keep treating zeros as missing, with the caveat that missing budgets much more likely to have been from small budget films in the first place.\n",
    "\n",
    "\n",
    "Acknowledgements\n",
    "This dataset was generated from The Movie Database API (https://www.kaggle.com/datasets/tmdb/themoviedb.org). This product uses the TMDb API but is not endorsed or certified by TMDb. Their API also provides access to data on many additional movies, actors and actresses, crew members, and TV shows. You can try it for yourself here: https://www.themoviedb.org/documentation/api.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ac88df",
   "metadata": {},
   "source": [
    "# Source Data Modifications: For Educational Purpses\n",
    "The source data was slightly modified for educational purposes. Many datasets found online tend to be relatively clean, with good naming conventions in the features and not many errors in the dataset. This probably is due to the dataset been through some level of data cleaning. In practice, databases and datasets may not be as clean of errors and issues.The modifications below provide other examples of data issues that can be typically found in databases and this notebook provides examples on how to identify and resolve them.\n",
    "\n",
    "For educational purposes the following errors were introduced in the source data:\n",
    "- Line 60, ID 5174, modified the budget from 1.4E8 to -1.4E8 to introduce a negatie error in the budget feature. A negative budget would be impossible and this would be an example of a potential data entry error.\n",
    "- Line 71, ID 44826, modified the runtime from 126 to NAN. A nan value is a missing value. \n",
    "- Line 4213, ID 74084, modified id from 74084 to 118340. The new ID also belongs to another movie. This is an example where a unique ID has a duplicate which by definition should not be possible. Note the title in this movie is giberish as well. User may need to decide and document if deleting both or keeping one. \n",
    "- Line 102, ID 4922, created a full duplicate of all values and inserted in as line 112.\n",
    "- Line 134, ID 19585, modifed runtime from 88 to 888. This would represent an error on the runtime. This may represent a data entry error.\n",
    "- Line 3115, ID 53953, modified runtime to string 'three'. This would represent a data entry error as this should be a number  (integer or float). In many cases the data entry user interface may have limits or a routine that checks that it uses the correct data type but in some other cases it does not.\n",
    "- Line 3672, ID 113406, modified runtime from 0 -124 to introduce a negative number error in the runtime feature. This may represent a data entry error.\n",
    "\n",
    "Also the data file was divided into three. This is to represent multiple data files and demonstrate how the data from multiple datafiles can be combined. This can represent datasets that were collected in different timeframes. Three files were created with the format \"XXXX-to-YYYY-ORIGINAL_FILENAME.csv\" where XXXX is the first release date year in the range and YYYY is the last release date year. The three files are: \"1916-to-1959-tmdb_5000_movies-Modified.csv\" (i.e., blank release dates are included in this one), \"1960-to-1989-tmdb_5000_movies-Modified.csv\", and \"1990-to-2017-tmdb_5000_movies-Modified.csv\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eccef9",
   "metadata": {},
   "source": [
    "# Library Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9003105f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "import re\n",
    "\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n",
    "from IPython.display import clear_output # Clears output in functions and Jupyter Notebook Interface.\n",
    "\n",
    "# Natural Language Processing (NLP) libraries\n",
    "# Used in the section on Text Normalization\n",
    "import nltk # NLP Library\n",
    "from nltk.stem import wordnet, WordNetLemmatizer, PorterStemmer # Word stemming and lematization\n",
    "from nltk.tokenize import RegexpTokenizer # Tokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer # Word stemming\n",
    "from nltk import pos_tag # For parts of speech\n",
    "from nltk import word_tokenize # To create tokens\n",
    "from nltk.corpus import wordnet, stopwords # For stop words\n",
    "\n",
    "# One time downloads. Remove hashtag and run once.\n",
    "#nltk.download('wordnet') # One time downlaod for NLTK 'wordnet'\n",
    "#nltk.download('stopwords') # One time downlaod for NLTK 'stopwords'.\n",
    "#nltk.download('averaged_perceptron_tagger') # One time download for NLTK 'average_perceptron_tagger'.\n",
    "#nltk.download('punkt') # One time download for NLTK 'punkt'. Remove the hashtag and run once.\n",
    "#nltk.download('omw-1.4') # One time download for NLTK 'omw-1.4'. Remove the hashtag and run once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da7b92c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: Oct/24/2022 \n",
      "Current Time: 18:57:33\n"
     ]
    }
   ],
   "source": [
    "print('Date: {:%b/%d/%Y} \\nCurrent Time: {}'.format(datetime.now(), \n",
    "                                                    datetime.now().strftime(\"%H:%M:%S\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6f801b",
   "metadata": {},
   "source": [
    "# Default Jupyter Notebook Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf87f52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None) # PD has a limit of 50 characters.  Removes limit and uses the full text.\n",
    "pd.options.display.float_format = \"{:.4f}\".format # Sets PD to displays float numbers as 4 decimal places."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f4b698",
   "metadata": {},
   "source": [
    "# Generic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fb06851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progress Bar Function. Used in loops.\n",
    "def progress_status(step, total_steps):\n",
    "    #Progress Status\n",
    "    clear_output(wait=True)\n",
    "    print(f\"Currently processing step: {step} of {total_steps}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35bb8aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_df(data): # Extracts the features as a dataframe\n",
    "    global dfcolumns_index\n",
    "    # Column selection for selecting columns in loops used in the data cleaning, visualization and model functions below.\n",
    "    dfcolumns = list(data.columns.values)\n",
    "    dfcolumns_index = pd.DataFrame(dfcolumns, columns=['column'])\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    return dfcolumns_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1b5d0a",
   "metadata": {},
   "source": [
    "#### Conversion of Multi-Class features/columns to one-hot-encoding.\n",
    "\n",
    "Parameters\n",
    "- df_name = Variable name of dataframe.\n",
    "- item_to_search = class_dict['feature'][i] # Relies on dictionary with unique classes. May also use a list of unique values.\n",
    "- col_to_search = 'original_feature' # Name of the feature in the original dataframe.\n",
    "- new_col_value = code_dict['feature'][i] # Creates new column with name of class and boolean (1 or 0) values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9ae579c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a one-hot-encoding columns for a class.\n",
    "def to_one_hot_encoding(df_name, item_to_search, col_to_search, new_col_value):\n",
    "    for i in range(len(df_name)): # Iterates thru the rows of the dataframe.\n",
    "         # Searches the value of interest (i.e., item_to_search) in the cell of interest.\n",
    "        if item_to_search in df_name.at[i, col_to_search]:\n",
    "            df_name.loc[i, new_col_value] = 1 # If the value is found in the column assigns 1.\n",
    "        else:\n",
    "            df_name.loc[i, new_col_value] = 0 # If the value is not found in the column assigns 0.\n",
    "    df_name[new_col_value] = df_name[new_col_value].astype('int') # Converts new column to integer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec46fa64",
   "metadata": {},
   "source": [
    "# Data Loading and Combining Multiple Files into One DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c099808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full List of files: ['1916-to-1959-tmdb_5000_movies-Modified.csv', '1960-to-1989-tmdb_5000_movies-Modified.csv', '1990-to-2017-tmdb_5000_movies-Modified.csv', 'README_tmdb_5000_movies-MOD_Notes.txt'].\n"
     ]
    }
   ],
   "source": [
    "all_files_list = os.listdir('./input_data/')\n",
    "print(f'Full List of files: {all_files_list}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b67be34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________________________________\n",
      "ALTERNATIVE 1:\n",
      "Files to be removed: ['README_tmdb_5000_movies-MOD_Notes.txt'].\n",
      "CSV files: ['1916-to-1959-tmdb_5000_movies-Modified.csv', '1960-to-1989-tmdb_5000_movies-Modified.csv', '1990-to-2017-tmdb_5000_movies-Modified.csv'].\n"
     ]
    }
   ],
   "source": [
    "# We only want to select the CSV files. There are a few approaches.\n",
    "# Alternative 1:\n",
    "# Using list comprehension and  for loop to iterate thru elements, select those that have \".csv\" in the filename.\n",
    "print(\"______________________________________________________\")\n",
    "print(\"ALTERNATIVE 1:\")\n",
    "removed_files = [element for element in all_files_list if \".csv\" not in element] # Files that don't have \".csv\"\n",
    "print(f'Files to be removed: {removed_files}.')\n",
    "csv_files = [element for element in all_files_list if \".csv\" in element] # Will only select csv files in directory.\n",
    "print(f'CSV files: {csv_files}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05cfff37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________________________________\n",
      "ALTERNATIVE 2: using filter and lambda to select '.csv' in the filename.\n",
      "CSV files: ['1916-to-1959-tmdb_5000_movies-Modified.csv', '1960-to-1989-tmdb_5000_movies-Modified.csv', '1990-to-2017-tmdb_5000_movies-Modified.csv'].\n"
     ]
    }
   ],
   "source": [
    "# Alternative 2: Use a filter and lambda to select \".csv\" in the filename.\n",
    "print(\"______________________________________________________\")\n",
    "print(\"ALTERNATIVE 2: using filter and lambda to select '.csv' in the filename.\")\n",
    "csv_files = list(filter(lambda x: '.csv' in x, csv_files)) # Will only select csv files in directory.\n",
    "print(f'CSV files: {csv_files}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe71d1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________________________________\n",
      "ALTERNATIVE 3: Regex re.search function to select '.csv' in the filename.\n",
      "CSV files: ['1916-to-1959-tmdb_5000_movies-Modified.csv', '1960-to-1989-tmdb_5000_movies-Modified.csv', '1990-to-2017-tmdb_5000_movies-Modified.csv'].\n"
     ]
    }
   ],
   "source": [
    "# Alternative 3: Use a filter and lambda to select \".csv\" in the filename, especially if case is sensitive.\n",
    "print(\"______________________________________________________\")\n",
    "print(\"ALTERNATIVE 3: Regex re.search function to select '.csv' in the filename.\")\n",
    "csv_files = [element for element in csv_files if re.search('.csv', element, flags = re.IGNORECASE)]\n",
    "print(f'CSV files: {csv_files}.')\n",
    "\n",
    "# May be other methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59981681",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_combined = pd.DataFrame() # Empty dataframe\n",
    "\n",
    "for element in range(len(csv_files)):\n",
    "    # The separator seems to be used we can extract the year range and original file name.\n",
    "    # We can use the element at position 0 as start year, at position 2 as end year.\n",
    "    file_name = csv_files[element].split(\"-\")\n",
    "    temp_df = pd.read_csv('./input_data/'+csv_files[element]) # Loading the data each file in the csv_files\n",
    "    temp_df['start_year'] = file_name[0] # Start year added at end of dataframe to all rows\n",
    "    # Recall if we want too select the last two integers of the YYXX string we could also use\n",
    "    # temp_df['start_year'] = file_name[0][2:4] # Selects string 3 and 4. For year 1916 would be 16.\n",
    "    temp_df['end_year'] = file_name[2] # End year added at end of dataframe to all rows\n",
    "    temp_df['source_filename'] = csv_files[element] # source_filename added at end of dataframe to all rows\n",
    "    # Concatenates the main DF with the temporary DF.\n",
    "    df_data_combined = pd.concat([df_data_combined, temp_df], axis=0, ignore_index = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20fb0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc467ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_combined.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32d192d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We really will not use the columns we created. For simplicity let's drop them.\n",
    "df_data_combined = df_data_combined.drop(columns=['start_year', 'end_year', 'source_filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bdd69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_combined.to_csv(r'.\\output_data\\tmdb_5000_movies-combined.csv', index = False, header=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afaa0e65",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607a1d34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# LOADING CSV FILE\n",
    "# Na_values may need to be reviewed as some datasets may include an accronym.\n",
    "# For example, 'NA' may be an abbreviation for 'North America'.\n",
    "df_data = pd.read_csv('./output_data/tmdb_5000_movies-combined.csv', \n",
    "                      encoding = \"utf-8-sig\",\n",
    "                      parse_dates=['release_date'],\n",
    "                      keep_default_na=False,\n",
    "                      na_values=['', '-1.#IND', '1.#QNAN', '1.#IND', '-1.#QNAN', '#N/A','N/A', '#NA', 'NULL', 'NaN', '-NaN', 'nan', '-nan']) \n",
    "\n",
    "# IF LOADING EXCEL FILE: use pd.read_excel.\n",
    "#df_data = pd.read_excel('.\\input_data\\FILE_NAME.xlsx', parse_dates=['Date', 'Final Date'])\n",
    "\n",
    "# Encoding \"cp1252\" or \"utf-8-sig\" used so that Excel does not create special characters. Standard Python is utf-8.\n",
    "# See reference for explanation https://stackoverflow.com/questions/57061645/why-is-%C3%82-printed-in-front-of-%C2%B1-when-code-is-run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546f520d",
   "metadata": {},
   "source": [
    "# Exploring DF Overall Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e8bbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the .head() function to prevew a sample record from the dataframe.\n",
    "df_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4230b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The dataframe (DF) has {df_data.shape[0]} rows and {df_data.shape[1]} features or columns.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0de8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#.info() function provides DF information including column name, number of non-nulls, and DType.\n",
    "df_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18eb0141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes on the above statistics:\n",
    "# Total of 4804 entries or records.\n",
    "# Homepage and tagline features have the most null values (i.e., least non-null).\n",
    "# Note that runtime is loaded as an object datatype.\n",
    "# Would have expected that movie runtime would be a number. We will need to explore this during data cleaning below.\n",
    "# Typically this may be caused by a data entry error where a string was used instead of a int or float.\n",
    "# There are two title related features \"original_title\" and \"title\". Let's explore below if we can drop one feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6542ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column values.\n",
    "df_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c751a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF of Columns with Index. \n",
    "# If Creating new derived features you may need to rerun this cell to add the new features prior to using it.\n",
    "# I typically use the DF of columns with index for selecting columns in loops.\n",
    "# Especially during data cleaning, derivation of features, visualization and model functions.\n",
    "get_features_df(data = df_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddfa03c",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "The purpose of the data cleaning is to prepare the data for input to visualizations, analysis, machine learning, and other activities. There are easy to identify errors that can be easy to correct. There are other types of errors that may be harder to identify and address during the data cleaning stage and may require input from a subject matter expert (SME) that is familiar with the dataset that can provide advise on how to identify other types of errors and how to handle during the data cleaning.\n",
    "\n",
    "In many of the examples here we will be droping features and rows. Depending on your use cases you will need to evaluate if it is appropriate to drop the data. I am a strong believer that if an error is found, the source data should be fixed in the database rather than cleaning stage. If it is not fixed there is always the possibility that either us as data scientist may use the data in the future or some other analyst using the source data may not find the error and their analysis outputs and results may be skewed or worst be wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43be34db",
   "metadata": {},
   "source": [
    "### Runtime Feature: Object vs. Should be a Number Issue\n",
    "As noted above, the runtime feature seemed to be detected as object data type by Pandas. Would have expected a number data type (e.g., integer or float)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b4ba75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initially may try to force Python to convert feature to a integer. \n",
    "#df_data['runtime'].astype(int) # This gives the following error \"cannot convert float NaN to integer\"\n",
    "# This is caused by potential of having np.nan as values. \n",
    "# Comment out this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c3f016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I may try to force Python to convert feature to a float.\n",
    "#df_data['runtime'].astype(float) # This gives the following error \"could not convert string to float: 'three'\"\n",
    "# These means that there is at least one string in the column (e.g., 'three')\n",
    "# Comment out this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0546df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first part of the line below converts runtime column to numeric and when there is an error leaves the value as null.\n",
    "# Then select the runtime feature and does a value_count.\n",
    "df_data[pd.to_numeric(df_data['runtime'], errors = 'coerce').isnull()]['runtime'].value_counts(dropna = False)\n",
    "# We can see that there is one count where the word three is used instead of a numeric value.\n",
    "# We can also see that there are three NaN or null values.\n",
    "# We need to remove these 4 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23b8cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are a few ways to remove rows in this case.\n",
    "# The approach below is best as it will remove any row that has a non-numeric value in the runtime.\n",
    "# If the data is updated in the future and used as input in this notebook this will work.\n",
    "# We can use the function above and select only those records or rows where there is no error as follows.\n",
    "df_data = df_data[pd.to_numeric(df_data['runtime'], errors = 'coerce').notnull()]\n",
    "# Then we want to specify to Python that this column is an integer.\n",
    "df_data['runtime'] = df_data['runtime'].astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c87bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.info() # We can check again the .info() in the data.\n",
    "# Confirm that runtime is now an integer data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34edbf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that there are now 4800 entries with the index from 0 to 4803. \n",
    "# Whenever dropping records should consider resetting the index of the dataframe.\n",
    "# In some cases not reseting the index may cause issues.\n",
    "# For example, when using ML algorihtms, a new DF with new information may be created that relies on indexes.\n",
    "# The new DF will start the index at 0.\n",
    "# If the index is not reset and you want to merge DF's the indexes will not match.\n",
    "df_data = df_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856173e1",
   "metadata": {},
   "source": [
    "### Title Features:\n",
    "There seem to be two title features. There may be reasons to keep both depending on the dataset and the goal but for our purpose it may be worth simplyfing to only one if we can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c65bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore how many titles are equal.\n",
    "(df_data['original_title'] == df_data['title']).value_counts()\n",
    "# Of all the values, a huge amount is the same.\n",
    "# One option, we could explore the differences.\n",
    "# There are so many titles that are the same to the original that will opt to drop one of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3478c6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the shape of the DF will help confirm the dropped feature.\n",
    "print(df_data.shape)\n",
    "# Let's drop title feature. I typically use comment blocks like this to document the reason for droping items.\n",
    "df_data = df_data.drop(columns=['title'])\n",
    "print(df_data.shape) # Verifying that the second element dropped from 20 to 18 features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eed4f6d",
   "metadata": {},
   "source": [
    "### Duplicated Values: Statistics and Cleaning\n",
    "Some datasets may have full duplicates (i.e., the full row repeated) or there may be unique values that are duplicated for some reason. The later may or may not be an issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da450ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates dataframe of records that are full duplicates.\n",
    "duplicates = df_data[df_data.duplicated(keep = False)].sort_values(by = 'id', ascending=True).copy() \n",
    "print(f'There are {duplicates.shape[0]} records were all features are duplicate.')\n",
    "duplicates.head(2) # Shows the duplicate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7aa66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing the shape of the data pre/post dropping duplicates allows to determine how many records were dropped. \n",
    "print(df_data.shape)\n",
    "# Drop duplicates, keeps the first record and reset index.\n",
    "df_data = df_data.drop_duplicates(keep = 'first').reset_index(drop=True)\n",
    "print(df_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21db2c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates duplicates and sorts it by ID.\n",
    "duplicate_ids = df_data[df_data['id'].duplicated(keep = False)].sort_values(by = 'id', ascending=True).copy() \n",
    "print(f'There are {duplicate_ids.shape[0]} records were the ID is a duplicate.')\n",
    "duplicate_ids.head(4) # Shows the duplicate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8915fb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_data.shape)\n",
    "# Drop duplicate based on 'id'. Keeps the first record and reset index.\n",
    "df_data = df_data.drop_duplicates(subset=['id'], keep = 'first').reset_index(drop=True)\n",
    "print(df_data.shape)\n",
    "df_data = df_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec91b038",
   "metadata": {},
   "source": [
    "### Null Values: Statistics and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ac8bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explores the total of null values in the dataframe.\n",
    "# There may be columsn we don't need and we may want to drop.\n",
    "df_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2761fdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that in many cases databases and datasets may not have as clean naming convention as the features in this data.\n",
    "# The feature names in this dataset is using 'snake case' where the words have a underscore.\n",
    "# To rename a column you could use the following.\n",
    "df_data.rename(columns={'homepage':'homepage_link',\n",
    "                        'tagline':'tagline_renamed',\n",
    "                        }, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da91c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50af06da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For our purpsoses, we don't need 'homepage' (now 'homepage_link') nor 'tagline' (now 'tagline_renamed') features.\n",
    "# We also may not need the 'keywords' feature.\n",
    "# Typically keywords may be used for information retrieval.\n",
    "# Performing information retrieval with a similarity search approach typically results in better outputs.\n",
    "# Similarity search will be demonstrated in the NLP notebook.\n",
    "# Let's drop them. I typically use comment blocks like this to document the reason for droping items.\n",
    "# Printing the shape of the DF will help confirm the two dropped featuers.\n",
    "print(df_data.shape)\n",
    "df_data = df_data.drop(columns=['homepage_link','tagline_renamed', 'keywords'])\n",
    "print(df_data.shape) # Verifying that the second element dropped from 20 to 18 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fe5929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'overview' feature has 3 null values. Let's explore.\n",
    "print(f\"The overview data has {df_data[df_data['overview'].isnull()].shape[0]} null values.\")\n",
    "df_data[df_data['overview'].isnull()].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d06ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative way of calling the 'overview' feature whcih has 3 null values. Let's explore.\n",
    "df_data[df_data.release_date.isnull()].head(2) # Note different way to call the feature.\n",
    "# This would not work if the data had spaces and would had to convert to snake case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84737874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring null values could be strings in unstructured data features.\n",
    "df_data[df_data['overview'] == 'N/A'].head()\n",
    "# There may be cases where the entry is a string (e.g., 'N/A', 'Not Applicable', etc.) instead of np.nan.\n",
    "# Sometimes I have missed things like this and found out when analyzing data.\n",
    "# For example, clustering most probably will finds a group of reports with such string.\n",
    "# In one case the data entry team was putting an entry that read \"Nothing to report in this year.\"\n",
    "# The data had thousands of records and it was not until I did the clustering of reports that I noticed.\n",
    "# There is no purpose on including a record like taht in the data analysis. \n",
    "# Had to go back to the data cleaning notebook. \n",
    "# I added a cell to clean out those records, document the finding and reran the whole notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2470e0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_data.shape) # Initial shape of the data.\n",
    "# We will use the 'overview' text for NLP. Since these are null values we will drop these records.\n",
    "df_data = df_data[df_data['overview'].notnull()]\n",
    "print(df_data.shape) # Shape of the data after droping in previous line.\n",
    "# We will also use the release data for calculating trends and will drop records with null values in this feature.\n",
    "df_data = df_data[df_data.release_date.notnull()]\n",
    "print(df_data.shape) # Shape of the data after droping in previous line.\n",
    "\n",
    "# We can also reset the index separately instead of the line above.\n",
    "df_data = df_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84196111",
   "metadata": {},
   "source": [
    "### Numerical Features: Statistics and Cleaning\n",
    "There are a few ways to explore the feature statistics in order to identify and decide which features to clean. We can use various functions including the .describe(), .hist() and sns.pairplot(). SNS Pairplot documentation at: https://seaborn.pydata.org/generated/seaborn.pairplot.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cda4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The .hist() or histogram function shows the distribution for eadh of the numerical features.\n",
    "# X-axis is the values in the feature and y-axis is how many or the counts.\n",
    "df_data.hist(figsize = [12, 12]);\n",
    "# A few observations:\n",
    "# A larg amount of entries close to the 0 budget.\n",
    "# Note on the range that the .hist() function is using for the x-values. \n",
    "# Release_date: most data is post 1980.\n",
    "# The x-axis of the runtime goes up to 800."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88e530e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sns.pairplot() plots the relationship between any-two features. See documentation for other useful parameters.\n",
    "# This can be used also for selecting which features may provide useful clusters or may be correlated.\n",
    "# Note that if there are too many numberical features may be difficult to visualize. \n",
    "sns.pairplot(df_data);\n",
    "# A few notes:\n",
    "# Initially may be easier to evaluate the feature only on the y-axis and identify outliers.\n",
    "# Budget row seems to show negative value that do not make sense regardless of the column.\n",
    "# Popularity seems to have a few outliers with very large values.\n",
    "# Revenue seems to show one large value that is outside of the cluster.\n",
    "# Runtime row seems to show one very large value and negative values that are outside of the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4916292f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.describe() # Statistics only for numerical features.\n",
    "# If we had run the .describe prior to addressing the runtime feature string issue, runtime would not have been included."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b0f3b1",
   "metadata": {},
   "source": [
    "Notes on the statistics of the movie numerical feature:\n",
    "\n",
    "'ID' is a unique value for each movies and statistics are not useful. Note that by default describe() function will caluclate statistics for all numerical columns.\n",
    "\n",
    "Zero values: The source data mentions that many features have zero values and it may be appropriate to treat these values as missing or null values.\n",
    "\n",
    "Budget feature:\n",
    "- Has a negative values which should not be possible. We may want to drop records that have negative budget. We may want to explore threshold were positive numbers in the budget make sense. For example, do we want to keep movies with a $0 budget?\n",
    "\n",
    "Runtime feature:\n",
    "- There seem to have movies with negative runtime which are also not possible.\n",
    "- Max runtime of 888 also seems troublesome. It is much higher than 107 minutes average, way beyond standard deviation and most probably an error.\n",
    "\n",
    "Revenue feature:\n",
    "- There seemed to be at least one value with extremely high value.\n",
    "\n",
    "Other features and statistics:\n",
    "- There may be other issues (e.g., with popularity, vote_average or vote_count).\n",
    "- Since these are more difficult to understand we may need a subject matter expert familiar with the data to identify issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66128d0e",
   "metadata": {},
   "source": [
    "For the purposes of this data cleaning we will assume that Budget, Revenue and Runtime features values of 0 are missing values and negative values are errors.\n",
    "\n",
    "There may be a few ways to approach and handle this:\n",
    "1. We could leave the data with these null/missing values. If using it for later analysis, visualizations, trending or as training data for ML we can filter it out. This will allow using the other features that are not null with the caveat that the records or entries with missing values may not be useful in some applications.\n",
    "2. We can decide to remove them.\n",
    "\n",
    "For the purposes of this notebook we will remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1147067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before filtering out the values less than 0. Let's explore the other identified issue of very large runtimes.\n",
    "# Lets do a value_counts but creating a few bins to be able to evaluate the range of the runtime values.\n",
    "# Note that including infinity in lower/upper bound may remove null values.\n",
    "# However, null values were evaluated previously and runtime feature had none.\n",
    "df_data['runtime'].value_counts(bins = [-np.inf, 0, 100, 200, 300, np.inf], sort = False, dropna = False) \n",
    "# Note that there are two entries that have runtime values larger than 300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0168e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can individually evaluate entries with runtime larger than 300 filetering data as follows.\n",
    "df_data[df_data['runtime'] > 300]\n",
    "# We can try to search the runtime to make sure it is correct.\n",
    "# Or we can decide that these are outliers that are not needed for our tasks.\n",
    "# In the case of the runtime = 888 this seems to be an error as is more than twice the value of the second highest value.\n",
    "# Lets remove any values that go beyond 350 minutes runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a97f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before filtering out the values let's evaluate the values of revenue as there seemed to be one very large value.\n",
    "# Initial bins were developed using the 25 and 75 percentiles and max values for revenue. \n",
    "df_data['revenue'].value_counts(bins = [-np.inf, 0, 2e7, 1e8, 1e9, 1.5e9, 2e9, 3e9, np.inf], \n",
    "                                sort = False, \n",
    "                                dropna = False)\n",
    "# Notes:\n",
    "# Even though graphically there was one potential extreme outlier, it does not seem as extreme when expressed numerically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1d135c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's filter for the max revenue item.\n",
    "df_data[df_data['revenue'] == df_data['revenue'].max()]\n",
    "# I am not a movie SME but do know the movie Avatar may be in the top highest grossing movies.\n",
    "# Will  not remove this datapoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46e231f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets use a data filtering approach to filter out the values that were determined to be dropped.\n",
    "print(df_data.shape)\n",
    "df_data = df_data.loc[(df_data['budget'] > 0) &\n",
    "                      (df_data['revenue'] > 0) &\n",
    "                      (df_data['runtime'] > 0) &\n",
    "                      (df_data['runtime'] <= 350)\n",
    "                     ].reset_index(drop=True)\n",
    "print(df_data.shape)\n",
    "# This will drop the dataframe from 4798 entries to 3226."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181442cd",
   "metadata": {},
   "source": [
    "### Non-Numerical Categorical Features\n",
    "This dataset has non-numerical categorical features in two formats. First, some features (e.g., original language, status) only have a single value. In others features an entry may have multiple values that may be applicable. In this dataset, these features with multiple values (e.g., genres, keywords, production_companies, production_countries and spoken_language) use a JSON or dictionary like format to capture the values.\n",
    "\n",
    "For the purposes of this notebook the statistics and cleaning will be separated in their own sections as the preparation and data manipulation will be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea81e00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the columns.\n",
    "df_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2937de9f",
   "metadata": {},
   "source": [
    "#### Non-Numerical Categorical Features: One-Hot-Encoding\n",
    "In some cases it may be beneficial to convert a data feature to one-hot-encoding format. Typically, a feature that has categorical values (e.g., genres, original language) can be converted to one-hot-encoding. The following sections explores cleaning of non-numerical categorical feataures and where appropriate convert to one-hot-encoding. The decision on which column to convert to one-hot-encoding comes with experience. One of the main reasons to convert to one-hot-encoding format is in preparation for features that will be used as part of post data cleaning analyses (e.g., classification tasks)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940964f4",
   "metadata": {},
   "source": [
    "#### Non-Numerical Categorical Features: Statistics and Cleaning\n",
    "This section looks at those non-numerical categorical features that only have one value per entry. These include status and original_language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2d7c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['status'].value_counts(normalize = False, sort = True, ascending = True, dropna = False).head(5)\n",
    "# Only one movie in post production and the rest are in released status.\n",
    "# Post production is clearly underrepresented.\n",
    "# The original data had a few more movies as post production and rumored but those entries were dropped in previous steps.\n",
    "# Since this is a feature that does not provide much information lets first drop this record and then the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb2115d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_data.shape)\n",
    "df_data = df_data[df_data.status != 'Post Production'] # Dropping rows different than 'Post Production'\n",
    "print(df_data.shape)\n",
    "\n",
    "# Remember when droping rows to reset index.\n",
    "df_data = df_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9434aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's drop the 'status' feature since it only has movies that have been released.\n",
    "print(df_data.shape)\n",
    "df_data = df_data.drop(columns=['status'])\n",
    "print(df_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66608c44",
   "metadata": {},
   "source": [
    "#### Non-Numerica Categorical Features JSON Like Format: Statistics and Cleaning\n",
    "\n",
    "Extract genre information as a feature or features. Note the dictionary has the id and the name. There may be a few ways to do this. Create a list of all the location 3 in the dictionary, then create all this list as columns and when contains put a 1. Extract List and Dictionary Data (note this may be similar with the multiple SPO). https://stackoverflow.com/questions/65544954/pandas-extract-values-from-a-column-with-multiple-list-of-dictionaries-and-spli\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfaee43",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_genres = [] # Initiate empty list\n",
    "\n",
    "# Iterates thru every row.\n",
    "for row in range(len(df_data)):\n",
    "    # Progress status. Will not make difference in these as processing a few thousand records is fast.\n",
    "    progress_status(step = row, total_steps = len(df_data)-1)\n",
    "    \n",
    "    cell_contents = json.loads(df_data.iloc[row]['genres']) # Loads cell data as json data.\n",
    "    # Consolidate new list with old list and remove duplicates with list(set())).\n",
    "    unique_genres = list(set((unique_genres + [item.get('name') for item in cell_contents])))\n",
    "    unique_genres.sort() # Sort list alphabetically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88b54b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are a total of {len(unique_genres)} unique genres: \\n{unique_genres}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6ec224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate thru the values of the unique_genres in order to create a column for each genre and the one hot encoding. \n",
    "for element in unique_genres:\n",
    "    # Progress status. This loop takes about a dozen seconds so you will see teh process increasing below.\n",
    "    progress_status(step = unique_genres.index(element), total_steps = len(unique_genres)-1)\n",
    "    \n",
    "    to_one_hot_encoding(df_name = df_data, \n",
    "                        item_to_search = element, \n",
    "                        col_to_search = 'genres', \n",
    "                        new_col_value = element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab073ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets do a sampling of the dataframe with genres column and the unique_genres columns. \n",
    "# This will allow to do a few manual spot checks on the one-hot-encoding.\n",
    "df_data.loc[:,['genres']+unique_genres].sample(n = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e21f09f",
   "metadata": {},
   "source": [
    "# Derived Features: Date Related\n",
    "For deriving date related features to work, the date feature (e.g., release_date) needs to be parsed as dates in either in the data loading step or afterwards setting the data type as date. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c68669f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d07368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show max() and min() date ranges.\n",
    "print('Date range is {:%d/%m/%Y} to {:%d/%m/%Y}'.format(df_data['release_date'].min(), df_data['release_date'].max()))\n",
    "# Because the date range is so large, I typically find it easier to evaluate issues with the year or month individually.\n",
    "# This helps find high level issues and limitations.\n",
    "# Trying to evaluate every single day individually may not be plausible.\n",
    "# The goal is to quickly identify potential data limitations and suppor cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3f9b16",
   "metadata": {},
   "source": [
    "### Derived Feature: Release Calendar Year (release_cy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f0cec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['release_cy'] = df_data['release_date'].dt.to_period('Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127f5317",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['release_cy'].value_counts(dropna = False).sort_index(ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645928d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['release_cy'].value_counts(dropna = False).sort_index().head(10)\n",
    "# There seem to be years not represented for example between 1917 to 1924 and 1928, 1931.\n",
    "# There are various ways to address these missing years if we would be calculating trends or forecasting.\n",
    "# This include but not limited to taking the average data of adjacent years. \n",
    "# For example lets say we wanted to calculate the revenue of 1928. We could use the average of 1927 and 1929.\n",
    "# We could also use forecasting using a regression and use the forecasted values for those years."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0050405d",
   "metadata": {},
   "source": [
    "### Derived Feature: Release Calendar Year Quarter (release_cy_quarter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea27d72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['release_cy_quarter'] = pd.PeriodIndex(df_data.release_date, freq = 'Q')\n",
    "\n",
    "# Alternative way to convert is using the approach of the CY.\n",
    "#df_data['release_cy_quarter'] = df_data['release_cy_quarter'].dt.to_period('Q')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a27e4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['release_cy_quarter'].value_counts(dropna = False).sort_index(ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb277f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['release_cy_quarter'].value_counts(dropna = False).sort_index().head(10)\n",
    "# Expect the same issue as the CY were there will be quarters that are not represented."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a05eb47",
   "metadata": {},
   "source": [
    "### Derived Feature: Release Calendar Year Month (release_cy_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faca3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['release_cy_month'] = pd.PeriodIndex(df_data.release_date, freq = 'M')\n",
    "\n",
    "# Alternative way to convert is using the approach of the CY.\n",
    "#df_data['release_cy_month'] = df_data['release_cy_month'].dt.to_period('M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bafbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['release_cy_month'].value_counts(dropna = False).sort_index(ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b54d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['release_cy_month'].value_counts(dropna = False).sort_index().head(10)\n",
    "# Expect the same issue as the CY were there will be quarters that are not represented."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4395b6",
   "metadata": {},
   "source": [
    "### Derived Feature: Fiscal Years \n",
    "To create Fiscal Year it is the same as above but adjust the period as follows:\n",
    "\n",
    "df_data['FY'] = df_data['DATE_FEATURE'].dt.to_period('A-SEP') # Annual-FY starting in September.\n",
    "\n",
    "Since movie releases do not have fiscal years there is no point on creating a 'release_fy' feature. However, for business data you may want to change. FY quarters would change the 'A-SEP' to 'Q-SEP'. Reading the datetime.to_period documentation will provide documentation on how to address."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb379ce",
   "metadata": {},
   "source": [
    "# Derived Feature: Box Office Earning: Revenue Plus Budget\n",
    "As explained in the analysis goals above, could use budget and revenue to calculate box office earnings. Per definition the  box_office_earnings seem to be equal to revenue plus budget. A subject matter expert (SME) familiar with the data may be able to provide further advice on which metrics would be useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaa9071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the derived box_office_earnings column from the addition of budget and revenue.\n",
    "df_data['box_office_earning'] = df_data['budget'] + df_data['revenue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd92a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['box_office_earning'].describe() \n",
    "# Note the minimum is very small which may mean that there are numbers in budget and revenue that may not be correct.\n",
    "# Determining a threshold on what is a credible box_office_earning would need to be determined by an SME.\n",
    "# Will not drop anymore entries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd447b2",
   "metadata": {},
   "source": [
    "# Derived Feature: Revenue Budget Ratio\n",
    "As explained in the analysis goals above, could use revenue and budget to calculate a revenue/budget ratio. A subject matter expert (SME) familiar with the data may be able to provide further advice on which metrics would be useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05c1722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the derived box_office_earnings column from the addition of budget and revenue.\n",
    "df_data['revenue_budget_ratio'] = df_data['revenue']/df_data['budget']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02358895",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['revenue_budget_ratio'].describe()\n",
    "# A revenue_budget_ratio of 0 makes sense only if the revenue is 0.\n",
    "# Determining a threshold on what is a credible revenue to budget ratio would need to be determined by an SME.\n",
    "# Will not drop anymore entries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbc3d84",
   "metadata": {},
   "source": [
    "### Non-Numerical Text Features: Statistics and Cleaning\n",
    "This section is to evaluate non-numerical text features and try to identify issues and errors. This is a little more difficult as these calculations and statistics may have to be developed. Some things that may be worthwhile to include:\n",
    "- Calculating the statistics of the lenghts (by word or characters) of text features (e.g., title and overviews). Evaluate if  there any extreme values (e.g., short overviews) that don't provide useful information and may be worth dropping?\n",
    "- Could try to find special characters within title and overview of the entries? This could imply that the wrong encoding was used when loading the data or potential errors in the data? \n",
    "\n",
    "Same or further analysis can be done after text normalization. There is also the potential to identify some issues during natural language processing analysis (e.g., wordcloud, text clustering, topic modeling) that may identify other entries that may be worth droping.\n",
    "\n",
    "In order to accomplish this we will do the following:\n",
    "- Let's create a new dataframe that includes some text statistics.\n",
    "- Calcualte word statistic include the number of words for title and for overview features\n",
    "- Calculate characters statistics include the number of characters for title and overview features\n",
    "- Can then use the \n",
    "- If need be can merge this new dataframe into the old one. Note that this is an example of where the reset_index will make a difference and could cause issues and be explained below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf41666a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create an empty dataframe for the statistics.\n",
    "df_data_text_stats = pd.DataFrame()\n",
    "df_data_text_stats.head() # Returns empty DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da6e874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates features for the character counts.\n",
    "# Applies a lambda function to develop the character count using the lenght of the string.\n",
    "df_data_text_stats['title_char_count'] = df_data['original_title'].apply(lambda x: len(str(x)))\n",
    "df_data_text_stats['overview_char_count'] = df_data['overview'].apply(lambda x: len(str(x)))\n",
    "\n",
    "# Creates features for the word counts.\n",
    "# There may be a few ways to do this. In this case we count the number of spaces.\n",
    "# If there is a cell with no spaces it means it has 1 word as we do not have null values in these two features.\n",
    "df_data_text_stats['title_word_count'] = df_data['original_title'].str.count(' ') + 1\n",
    "df_data_text_stats['overview_word_count'] = df_data['overview'].str.count(' ') + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b18558",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_text_stats.tail() # DF of characters and word counts. Showing the tail of DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1df8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_text_stats.describe(percentiles = [0.05, 0.25, 0.5, 0.75]) \n",
    "# Using the describe function with additional 5% percentile to evaluate stats of the text data.\n",
    "# In this case, very low numbers may be worth evaluating.\n",
    "# Depending on the dataset the thresholds to evaluate may change.\n",
    "# Determining the threshols should be an SME familiar with the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c91e449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Title Character Stats:\n",
    "# Expect the titles to be short. The min number of characters in the title is 1.\n",
    "# The loest 5th percentile has 5 characters. \n",
    "# We may want to evaluate by filtering the lowest 5th percentile, sort ascending and see if there is any with errors.\n",
    "\n",
    "# Overview Character Stats:\n",
    "# We can see that the minimum character count in the overview is 41 characters.\n",
    "# The mean is 303 characters.\n",
    "# The lowest 5th percentile has 41 characters which although short seems plausible.\n",
    "\n",
    "# Title Character Stats:\n",
    "# Expect titles to have only one word.\n",
    "\n",
    "# Overview Word Stats:\n",
    "# Min word lenght in an overview is 6 words. This may be one we may want to check.\n",
    "# We may want to check all entries with an overview shorter than 10 words.\n",
    "# Note that we will have another opportunity to explore the normalized overview text data later. \n",
    "\n",
    "# Since we have already cleaned out many entries we may have already achieved a good dataset.\n",
    "# There may not be many errors in the last features we are evaluating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515ffed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Considerations for concatenating the df_data_text_stats and add these features to the df_data: \n",
    "# We need to make sure indexes match.\n",
    "# This is one example where the reset_index needs to be used, otherwise the indexes may not match.\n",
    "# Concatenating one DF with the other will result in matching the wrong entries with the wrong data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c03a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make sure that our data matches.\n",
    "print('df_data') \n",
    "print(f'{df_data.shape[0]} rows \\n{df_data.index}.')\n",
    "print('____________________________________________')\n",
    "print('df_data_text_stats')\n",
    "print(f'{df_data_text_stats.shape[0]} rows \\n{df_data_text_stats.index}.')\n",
    "\n",
    "# Let's use an If statement: If the rows or the index stop do not match the Warning will be printed.\n",
    "if (df_data.shape[0] != \n",
    "    df_data_text_stats.shape[0]) or (df_data.index[1] != \n",
    "                                     df_data_text_stats.index[1]):\n",
    "    print('WARNING! WARNING! WARNING! INDEXES OF A DF MAY NEED RESETTING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d64e9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating the two dataframes will allow us to filter the main dataframe with word/character statistic values.\n",
    "# Concatenates the two dataframes. Adds the two df_data_text_stats as columns in the end.\n",
    "df_data = pd.concat([df_data, df_data_text_stats], axis = 1, ignore_index=False)\n",
    "# Could also use the .insert() function.\n",
    "# Copy the columns of interest from df_data_text_stats and insert them into the df_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3c6e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data of the four features and spot check.\n",
    "df_data.loc[:,['original_title', 'overview']+df_data_text_stats.columns.to_list()].sample(n = 3)\n",
    "# The .samle() function is similar to the .head but randomly selects rows.\n",
    "# This allows us to do random spot checks in 5 rows in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1976fafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {df_data[df_data['overview_word_count'] <= 10].shape[0]} entries with overview less than 10 words.\")\n",
    "df_data[df_data['overview_word_count'] <= 10].head(10)\n",
    "# Checking these overview narratives even though short may still be acceptable.\n",
    "# Will not drop any entries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334caf99",
   "metadata": {},
   "source": [
    "# Text Data Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f719e02d",
   "metadata": {},
   "source": [
    "### Derived Feature: Combination of Original Title and Overview\n",
    "Before we perform text data normalization we will combine the text of the 'original_title' and the text of the 'overview'. This will result in a narrative that will read \"Title of the movie: Overview\". We can then use this composite text to do analytics later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b0a6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['original_title_overview'] = df_data['original_title'] + ': ' + df_data['overview']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f841820c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's spot check a few samples.\n",
    "df_data[['original_title', 'overview', 'original_title_overview']].sample(n = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ff0af0",
   "metadata": {},
   "source": [
    "# Text Normalization\n",
    "This part may be considered part of Natural Language Processing (NLP). However, since I am creating a derived feature with normalized text I typically include this as part of my data cleaning, data wrangling, data transformation and data preparation. This step on text normalization includes deriving a feature with the lemmatized text and the stemmed text. Both, lemmatization and stemming are techniques to reduce words to their roots. Also, part of the text normalization step, there are stop words that are removed.\n",
    "\n",
    "This new features can be used for wordcloud, Bag of Word models which can be used in text clustering, similarity searching and other NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c5101d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords to add need to be developed by a SME familiar with the corpus.\n",
    "# These stopwords_to_add are added in pre-processing as part of the text normalization function.\n",
    "stopwords_to_add = [''] # If you wanted to add a corpus related stopword list add them here.\n",
    "\n",
    "# NLTK library stopwords\n",
    "stopwords_custom = stopwords.words('english') + [x.lower() for x in stopwords_to_add]\n",
    "\n",
    "# In some cases you want to consider 2-grams especially with the word 'no', 'not', 'nor'.\n",
    "# For example 'no fire'.  Removing the word 'no' from the stopwords list allows this to occur.\n",
    "remove_as_stopword = ['no', 'not', 'nor']\n",
    "stopwords_custom = list(filter(lambda w: w not in remove_as_stopword, stopwords_custom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f8eb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stopwords_custom) # Comment out to see list of stopwords_custom.\n",
    "# You can find these in the NLTK stopwords documentation as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70a216f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization of text. \n",
    "def text_normalization(text, word_reduction_method):\n",
    "    text = str(text) # Convert narrative to string.\n",
    "    df = pd.DataFrame({'': [text]}) # Converts narrative to a dataframe format use replace functions.\n",
    "    df[''] = df[''].str.lower() # Covert narrative to lower case.\n",
    "    df[''] = df[''].str.replace(\"\\d+\", \" \", regex = True) # Remove numbers\n",
    "    df[''] = df[''].str.replace(\"[^\\w\\s]\", \" \", regex = True) # Remove special characters\n",
    "    df[''] = df[''].str.replace(\"_\", \" \", regex = True) # Remove underscores characters\n",
    "    df[''] = df[''].str.replace('\\s+', ' ', regex = True) # Replace multiple spaces with single\n",
    "    text = str(df[0:1]) # Extracts narrative from dataframe.\n",
    "    tokenizer = RegexpTokenizer(r'\\w+') # Tokenizer.\n",
    "    tokens = tokenizer.tokenize(text) # Tokenize words.\n",
    "    filtered_words = [w for w in tokens if len(w) > 1 if not w in stopwords_custom] # Note remove words of 1 letter only. Can increase to higher value as needed.\n",
    "    if word_reduction_method == 'Lemmatization':\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        reduced_words=[lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in filtered_words] # Lemmatization.  The second argument is the POS tag.\n",
    "    if word_reduction_method == 'Stemming':\n",
    "        stemmer = PorterStemmer() # Stemming also could make the word unreadable but is faster than lemmatization.\n",
    "        reduced_words=[stemmer.stem(w) for w in filtered_words]\n",
    "    return \" \".join(reduced_words) # Join words with space.\n",
    "\n",
    "def get_wordnet_pos(word): # Reference: https://www.machinelearningplus.com/nlp/lemmatization-examples-python/#wordnetlemmatizer\n",
    "    #\"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d533e5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell may take awhile to run depending on the number of entries and lenght of entries being normalized.\n",
    "# In this dataset takes about 3 minutes.\n",
    "# It reads the data in the \"original_title_overview\" and normalizes the text using lemmatization.\n",
    "# It creates a new feature called norm_text_lemma.\n",
    "\n",
    "# Note that the progress_status function only works within loops.\n",
    "# In this case just printing the time before and after to estimate how long it takes.\n",
    "print(datetime.now())\n",
    "df_data['norm_text_lemma'] = df_data['original_title_overview'].apply(text_normalization, \n",
    "                                                                      word_reduction_method = 'Lemmatization')\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51370cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell may take awhile to run depending on the number of entries and lenght of entries being normalized.\n",
    "# In this dataset takes about 1 minutes. Note that stemming is much faster than lemmatization.\n",
    "# It reads the data in the \"original_title_overview\" and normalizes the text using stemming.\n",
    "# It creates a new feature called norm_text_stem.\n",
    "\n",
    "# Note that the progress_status function only works within loops.\n",
    "# In this case we are calculating a start and end time and substract to estimat total time to create the new features.\n",
    "# Alternatively could iterate thru all cells, apply text_normalization and use the progress_status.\n",
    "start_time = datetime.now()\n",
    "df_data['norm_text_stem'] = df_data['original_title_overview'].apply(text_normalization, \n",
    "                                                                      word_reduction_method = 'Stemming')\n",
    "end_time = datetime.now()\n",
    "total_time = end_time - start_time\n",
    "\n",
    "print('Total time to derive norm_text_lemma feature is: {} in hours:mins:secs'.format(str(total_time).split('.', 1)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388932bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows the original_title_overview feature and the derived features with normalized, lemmatized and stemmed text.\n",
    "df_data[['original_title_overview', 'norm_text_lemma', 'norm_text_stem']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff4fd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally let's check that after text normalization there are no null values in the norm_text columns.\n",
    "df_data[['original_title_overview', 'norm_text_lemma', 'norm_text_stem']].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622625d1",
   "metadata": {},
   "source": [
    "# Export Final DataFrame\n",
    "Exporting the final cleaned dataframe which can then be used as input into the other notebooks or shared with other stakeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca6238d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO EXPORT AS EXCEL\n",
    "#df_data.to_excel (r'.\\output_data\\df_data_clean.xlsx', index = False, header=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0110279f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO EXPORT AS CSV\n",
    "df_data.to_csv (r'.\\output_data\\df_data_clean.csv', encoding='utf-8-sig', index = False, header=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a01720",
   "metadata": {},
   "source": [
    "# Notebook End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801cc353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8041805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eddd81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
